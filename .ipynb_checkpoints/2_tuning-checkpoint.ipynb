{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425, 32)\n",
      "Index(['class_a', 'class_b', 'relation', 'ans_yamasaki', 'ans_ueda',\n",
      "       'ans_daisuke', 'ans_hosomi', 'ans_kazuma', 'ans_sugi', 'type', 'title',\n",
      "       'source', 'sim_by_mcg', 'is_a_by_mcg', 'lexvec_sim', 'wiki2vec_sim',\n",
      "       'wn_hu_average', 'wn_pu_average', 'wn_hu_best', 'wn_pu_best',\n",
      "       'wn_sim_path', 'wn_sim_lch', 'wn_sim_wup', 'wn_sim_res', 'wn_sim_jcn',\n",
      "       'wn_sim_lin', 'has_same_word', 'is_include_word', 'bert_sim',\n",
      "       'lexvec_diff', 'wiki2vec_diff', 'bert_diff'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#dataのインポート+シャッフル\n",
    "data = pd.read_csv(\"./dataset/exp_data.csv\",index_col = 0).sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "print(data.shape)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#カラム配列定義\n",
    "#特徴量カラム\n",
    "feature_columns =  [\"sim_by_mcg\",\"is_a_by_mcg\",\"lexvec_sim\",\"wiki2vec_sim\",\"wn_hu_average\",\n",
    "                \"wn_pu_average\",\"wn_sim_path\",\"wn_sim_wup\",\n",
    "                \"is_include_word\",\"bert_sim\",\"lexvec_diff\",\"wiki2vec_diff\",\"bert_diff\"]\n",
    "\n",
    "\n",
    "#通常の特徴量ラベル\n",
    "X_normal_columns = [\"sim_by_mcg\",\"is_a_by_mcg\",\"lexvec_sim\",\"wiki2vec_sim\",\"wn_hu_average\",\n",
    "                \"wn_pu_average\",\"wn_sim_path\",\"wn_sim_wup\",\"is_include_word\",\"bert_sim\"]\n",
    "\n",
    "#ベクトル系特徴量ラベル\n",
    "pca_di={\"lexvec_diff\":30,\"wiki2vec_diff\":30,\"bert_diff\":3}\n",
    "\n",
    "#被験者実験データ\n",
    "ans_columns = [ 'ans_ueda',\n",
    "       'ans_daisuke', 'ans_kazuma', 'ans_sugi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s1' 'h2' 'h1' 's2' 'k'],[3 1 0 4 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 欠損値は最頻値補完を行う\n",
    "def fill_na_mean(data):\n",
    "    data.loc[:,\"sim_by_mcg\"] = data.loc[:,\"sim_by_mcg\"].fillna(data.loc[:,\"sim_by_mcg\"].mode()[0])\n",
    "    data.loc[:,\"is_a_by_mcg\"] = data.loc[:,\"is_a_by_mcg\"].fillna(data.loc[:,\"is_a_by_mcg\"].mode()[0])\n",
    "    data.loc[:,\"wiki2vec_sim\"] = data.loc[:,\"wiki2vec_sim\"].fillna(data.loc[:,\"wiki2vec_sim\"].mode()[0])\n",
    "    data.loc[:,\"wiki2vec_diff\"] = data.loc[:,\"wiki2vec_diff\"].fillna(data.loc[:,\"wiki2vec_diff\"].mode()[0])\n",
    "    return data\n",
    "data = fill_na_mean(data)\n",
    "#クラスラベルエンコーディング\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def label_encorder(data):\n",
    "    le = LabelEncoder()\n",
    "    encoded = le.fit_transform(data['relation'].values)\n",
    "    for ans_c in ans_columns:\n",
    "        temp_label = le.transform(data[ans_c].values)\n",
    "        data[ans_c] = temp_label\n",
    "    data.loc[:,'relation_label'] = encoded\n",
    "    print(str(data[\"relation\"].unique())+\",\"+str(le.transform(data[\"relation\"].unique())))\n",
    "    return data\n",
    "data = label_encorder(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量の選別 wn_sim_jcnは値がバグっているので使わない(infinityが入っている)\n",
    "X_data = data.loc[:,feature_columns]\n",
    "# yの定義\n",
    "y = data.loc[:,\"relation_label\"].values.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#diffラベルのpca_ssモデル trainとtest arrを返す\n",
    "def ss_pca(train,pca_n):\n",
    "    scaler = StandardScaler()\n",
    "    train=scaler.fit_transform(train)\n",
    "    pca = PCA(n_components = pca_n)\n",
    "    train = pca.fit_transform(train)\n",
    "    return train\n",
    "#str_arrをarrに変換\n",
    "def convert_to_arr(vec_str):\n",
    "    vec_arr = None\n",
    "    for s in vec_str:\n",
    "        if vec_arr is None:\n",
    "            vec_arr =np.array(s[1:-1].split(\", \")).astype(\"float32\")\n",
    "        else:\n",
    "            vec_arr = np.block([[vec_arr],[np.array(s[1:-1].split(\", \")).astype(\"float32\")]])\n",
    "    return vec_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通常系の特徴量と、vec系の特徴量に分割し、vecにはpcaを行って最後に結合する\n",
    "X = X_data.loc[:,X_normal_columns].values.astype(\"float64\").astype(\"float64\")\n",
    "for c_name, n in pca_di.items():\n",
    "    vec_arr = convert_to_arr(X_data.loc[:,c_name])\n",
    "    pca_arr = ss_pca(vec_arr,n)\n",
    "    X = np.concatenate([X, pca_arr], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  \"n_estimators\": [50, 100, 200,300,400,500,600,700,800],\n",
    "  \"max_depth\": [5, 10,15,20],\n",
    "  \"learning_rate\": [0.01,0.05,0.1],\n",
    "    'subsample': [0.5, 0.9],\n",
    "    'min_child_weight':[1, 5]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=25,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=1500, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#普通に学習を行う\n",
    "#XGBOOST\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "model = xgb.XGBClassifier(n_estimators=1500,max_depth=25,learning_rate=0.01,random_state=0)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = X_normal_columns\n",
    "for name, n in pca_di.items():\n",
    "    features.extend([name+str(i) for i in range(n)])\n",
    "    \n",
    "mapper = {'f{0}'.format(i): v for i, v in enumerate(features)}\n",
    "mapped = {mapper[k]: v for k, v in model._Booster.get_fscore().items()}\n",
    "imp_tapple = sorted(mapped.items(),key=lambda x:x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
