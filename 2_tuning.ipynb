{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425, 32)\n",
      "Index(['class_a', 'class_b', 'relation', 'ans_yamasaki', 'ans_ueda',\n",
      "       'ans_daisuke', 'ans_hosomi', 'ans_kazuma', 'ans_sugi', 'type', 'title',\n",
      "       'source', 'sim_by_mcg', 'is_a_by_mcg', 'lexvec_sim', 'wiki2vec_sim',\n",
      "       'wn_hu_average', 'wn_pu_average', 'wn_hu_best', 'wn_pu_best',\n",
      "       'wn_sim_path', 'wn_sim_lch', 'wn_sim_wup', 'wn_sim_res', 'wn_sim_jcn',\n",
      "       'wn_sim_lin', 'has_same_word', 'is_include_word', 'bert_sim',\n",
      "       'lexvec_diff', 'wiki2vec_diff', 'bert_diff'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#dataのインポート+シャッフル\n",
    "data = pd.read_csv(\"./dataset/exp_data.csv\",index_col = 0).sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "print(data.shape)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#カラム配列定義\n",
    "#特徴量カラム\n",
    "feature_columns =  [\"sim_by_mcg\",\"is_a_by_mcg\",\"lexvec_sim\",\"wiki2vec_sim\",\"wn_hu_average\",\n",
    "                \"wn_pu_average\",\"wn_sim_path\",\"wn_sim_wup\",\n",
    "                \"is_include_word\",\"bert_sim\",\"lexvec_diff\",\"wiki2vec_diff\",\"bert_diff\"]\n",
    "\n",
    "\n",
    "#通常の特徴量ラベル\n",
    "X_normal_columns = [\"sim_by_mcg\",\"is_a_by_mcg\",\"lexvec_sim\",\"wiki2vec_sim\",\"wn_hu_average\",\n",
    "                \"wn_pu_average\",\"wn_sim_path\",\"wn_sim_wup\",\"is_include_word\",\"bert_sim\"]\n",
    "\n",
    "#ベクトル系特徴量ラベル\n",
    "pca_di={\"lexvec_diff\":30,\"wiki2vec_diff\":30,\"bert_diff\":3}\n",
    "\n",
    "#被験者実験データ\n",
    "ans_columns = [ 'ans_ueda',\n",
    "       'ans_daisuke', 'ans_kazuma', 'ans_sugi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s1' 'h2' 'h1' 's2' 'k'],[3 1 0 4 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 欠損値は最頻値補完を行う\n",
    "def fill_na_mean(data):\n",
    "    data.loc[:,\"sim_by_mcg\"] = data.loc[:,\"sim_by_mcg\"].fillna(data.loc[:,\"sim_by_mcg\"].mode()[0])\n",
    "    data.loc[:,\"is_a_by_mcg\"] = data.loc[:,\"is_a_by_mcg\"].fillna(data.loc[:,\"is_a_by_mcg\"].mode()[0])\n",
    "    data.loc[:,\"wiki2vec_sim\"] = data.loc[:,\"wiki2vec_sim\"].fillna(data.loc[:,\"wiki2vec_sim\"].mode()[0])\n",
    "    data.loc[:,\"wiki2vec_diff\"] = data.loc[:,\"wiki2vec_diff\"].fillna(data.loc[:,\"wiki2vec_diff\"].mode()[0])\n",
    "    return data\n",
    "data = fill_na_mean(data)\n",
    "#クラスラベルエンコーディング\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def label_encorder(data):\n",
    "    le = LabelEncoder()\n",
    "    encoded = le.fit_transform(data['relation'].values)\n",
    "    for ans_c in ans_columns:\n",
    "        temp_label = le.transform(data[ans_c].values)\n",
    "        data[ans_c] = temp_label\n",
    "    data.loc[:,'relation_label'] = encoded\n",
    "    print(str(data[\"relation\"].unique())+\",\"+str(le.transform(data[\"relation\"].unique())))\n",
    "    return data\n",
    "data = label_encorder(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量の選別 wn_sim_jcnは値がバグっているので使わない(infinityが入っている)\n",
    "X_data = data.loc[:,feature_columns]\n",
    "# yの定義\n",
    "y = data.loc[:,\"relation_label\"].values.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#diffラベルのpca_ssモデル trainとtest arrを返す\n",
    "def ss_pca(train,pca_n):\n",
    "    scaler = StandardScaler()\n",
    "    train=scaler.fit_transform(train)\n",
    "    pca = PCA(n_components = pca_n)\n",
    "    train = pca.fit_transform(train)\n",
    "    return train\n",
    "#str_arrをarrに変換\n",
    "def convert_to_arr(vec_str):\n",
    "    vec_arr = None\n",
    "    for s in vec_str:\n",
    "        if vec_arr is None:\n",
    "            vec_arr =np.array(s[1:-1].split(\", \")).astype(\"float32\")\n",
    "        else:\n",
    "            vec_arr = np.block([[vec_arr],[np.array(s[1:-1].split(\", \")).astype(\"float32\")]])\n",
    "    return vec_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通常系の特徴量と、vec系の特徴量に分割し、vecにはpcaを行って最後に結合する\n",
    "X = X_data.loc[:,X_normal_columns].values.astype(\"float64\").astype(\"float64\")\n",
    "for c_name, n in pca_di.items():\n",
    "    vec_arr = convert_to_arr(X_data.loc[:,c_name])\n",
    "    pca_arr = ss_pca(vec_arr,n)\n",
    "    X = np.concatenate([X, pca_arr], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-14 15:47:03,108]\u001b[0m A new study created in memory with name: no-name-6ae60053-dadc-47f2-a7c3-680ae26ffe01\u001b[0m\n",
      "/Users/akihito/.pyenv/versions/3.8.5/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 82 members, which is less than n_splits=90.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#1.目的関数の定義\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0.01, 1)\n",
    "    max_depth = trial.suggest_int('max_depth', 2,20)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.01, 5)\n",
    "    subsample = trial.suggest_uniform('subsample', 0.01, 1)\n",
    "    eta = trial.suggest_uniform(\"eta\",0.1,0.5)\n",
    "    clf1 = XGBClassifier(learning_rate=learning_rate,\n",
    "                         max_depth=max_depth,\n",
    "                         n_estimators=n_estimators,\n",
    "                         min_child_weight=min_child_weight,\n",
    "                         gamma=gamma)\n",
    "    \n",
    "    score = cross_val_score(clf1, X, y, cv=90)\n",
    "    print(score)\n",
    "    return np.mean(score)\n",
    "\n",
    "#2.Optunaオブジェクトを作成\n",
    "study = optuna.create_study(direction='maximize')\n",
    "#3.Optunaでパラメータ探索\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "#4.ベストパラメータ表示\n",
    "study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(opt, n_trials=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
